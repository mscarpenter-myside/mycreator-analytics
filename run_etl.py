#!/usr/bin/env python3
"""
MyCreator Analytics ETL - Multi-Workspace
==========================================

Extrai dados de performance das 4 cidades configuradas:
- Florian√≥polis: 696e75c20f3354d37f074866
- Florian√≥polis Continente: 696689afcddd41ec6a024adb  
- Goi√¢nia: 696689f3c04f3fefdc0118cd
- MyCreator: 68fbfe91e94c0946d103643d

Fluxo:
1. Extract: Busca dados da API MyCreator (multi-workspace)
2. Transform: Converte PostData para DataFrame pandas
3. Load: Atualiza Google Sheets

Uso: python run_etl.py
"""
import logging
import sys
from datetime import datetime
from dataclasses import asdict

import pandas as pd

from src.config import get_config, setup_logging
from src.extract import MyCreatorExtractor, TARGET_WORKSPACES
from src.load import load_to_sheets


def run_etl() -> bool:
    """Executa o pipeline ETL completo."""
    
    # =========================================================================
    # SETUP
    # =========================================================================
    config = get_config()
    logger = setup_logging(debug=config.debug_mode)
    start_time = datetime.now()
    
    logger.info("=" * 60)
    logger.info("üöÄ INICIANDO ETL MyCreator Analytics")
    logger.info(f"üìÖ Data/Hora: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    logger.info(f"üéØ Workspaces: {len(TARGET_WORKSPACES)} cidades")
    for ws in TARGET_WORKSPACES:
        logger.info(f"   ‚Ä¢ {ws['name']}: {ws['id']}")
    logger.info("=" * 60)
    
    try:
        # =====================================================================
        # ETAPA 1: EXTRACT
        # =====================================================================
        logger.info("\nüì° ETAPA 1: EXTRA√á√ÉO")
        extractor = MyCreatorExtractor(config)
        
        # Extrai de todos os workspaces (lista fixa em TARGET_WORKSPACES)
        all_posts = extractor.extract_from_workspaces()
        
        if not all_posts:
            logger.warning("‚ö†Ô∏è Nenhum post extra√≠do de nenhum workspace.")
            return False
        
        # =====================================================================
        # ETAPA 2: TRANSFORM
        # =====================================================================
        logger.info("\nüîÑ ETAPA 2: TRANSFORMA√á√ÉO")
        
        # Converte lista de dataclass para DataFrame
        records = [asdict(post) for post in all_posts]
        df = pd.DataFrame(records)
        
        # =================================================================
        # MAPEAMENTO DE COLUNAS - ORDEM DEFINITIVA PARA DIRETORIA
        # =================================================================
        # Primeira coluna: Cidade | √öltima coluna: Timestamp de Atualiza√ß√£o
        column_mapping = {
            # IDENTIFICA√á√ÉO (primeira: Cidade, segunda: Data de Publica√ß√£o)
            "workspace_name": "Cidade",
            "published_at": "Data de Publica√ß√£o",
            "platform": "Rede Social",  # Instagram, Facebook, etc
            "profile_name": "Perfil",
            "post_type": "Tipo",  # REELS, FEED, STORY
            
            # CONTE√öDO
            "title": "T√≠tulo",  # Nome do v√≠deo/publica√ß√£o
            "caption": "Legenda",
            
            # ENGAJAMENTO
            "likes": "Likes",
            "comments": "Coment√°rios",
            "saves": "Salvos",
            "shares": "Compartilhamentos",
            
            # PERFORMANCE
            "reach": "Alcance",
            "impressions": "Impress√µes",
            "plays": "Plays",
            
            # T√âCNICO (√∫ltima: Timestamp de Atualiza√ß√£o)
            "permalink": "Link",
            "external_id": "ID Instagram",
            "internal_id": "ID Interno",
            "analytics_error": "Status Dados",
            "extraction_timestamp": "Timestamp de Atualiza√ß√£o",
        }
        
        # Seleciona e renomeia colunas existentes (preserva ordem do dict)
        columns_to_export = [col for col in column_mapping.keys() if col in df.columns]
        df_final = df[columns_to_export].rename(columns=column_mapping)
        
        # Formata Data de Publica√ß√£o apenas como data (DD/MM/YYYY)
        if "Data de Publica√ß√£o" in df_final.columns:
            df_final["Data de Publica√ß√£o"] = pd.to_datetime(
                df_final["Data de Publica√ß√£o"], errors='coerce'
            ).dt.strftime("%d/%m/%Y")
        
        # Ordena por cidade e data (mais recentes primeiro)
        if "Data de Publica√ß√£o" in df_final.columns and "Cidade" in df_final.columns:
            df_final = df_final.sort_values(
                by=["Cidade", "Data de Publica√ß√£o"], 
                ascending=[True, False]
            )
        
        # Estat√≠sticas
        total_posts = len(df_final)
        total_likes = int(df_final["Likes"].sum()) if "Likes" in df_final.columns else 0
        total_reach = int(df_final["Alcance"].sum()) if "Alcance" in df_final.columns else 0
        total_comments = int(df_final["Coment√°rios"].sum()) if "Coment√°rios" in df_final.columns else 0
        
        logger.info(f"üìä Total: {total_posts} posts")
        logger.info(f"‚ù§Ô∏è Likes: {total_likes:,}")
        logger.info(f"üëÅÔ∏è Alcance: {total_reach:,}")
        logger.info(f"üí¨ Coment√°rios: {total_comments:,}")
        
        # Resumo por cidade
        logger.info("\nüìà RESUMO POR CIDADE:")
        for cidade in df_final["Cidade"].unique():
            df_cidade = df_final[df_final["Cidade"] == cidade]
            logger.info(f"   ‚Ä¢ {cidade}: {len(df_cidade)} posts | {int(df_cidade['Likes'].sum())} likes")
        
        # =====================================================================
        # ETAPA 3: LOAD (GOOGLE SHEETS)
        # =====================================================================
        logger.info("\nüì§ ETAPA 3: CARGA NO GOOGLE SHEETS")
        logger.info(f"üìë Sheet ID: {config.google_sheet_id}")
        logger.info(f"üìë Aba: {config.sheet_tab_name}")
        logger.info(f"üìù Modo: {config.write_mode}")
        
        sheets_success = load_to_sheets(df_final, config)
        
        if not sheets_success:
            logger.error("‚ùå Falha ao atualizar Google Sheets!")
            return False
        
        logger.info("‚úÖ Google Sheets atualizado com sucesso!")
        
        # =====================================================================
        # RESUMO FINAL
        # =====================================================================
        duration = (datetime.now() - start_time).total_seconds()
        
        logger.info("\n" + "=" * 60)
        logger.info("üèÅ ETL CONCLU√çDO COM SUCESSO!")
        logger.info(f"‚è±Ô∏è Dura√ß√£o: {duration:.2f} segundos")
        logger.info(f"üìä Posts processados: {total_posts}")
        logger.info(f"üìÑ Sheets: https://docs.google.com/spreadsheets/d/{config.google_sheet_id}")
        logger.info("=" * 60)
        
        return True
        
    except Exception as e:
        logger.exception(f"‚ùå Erro fatal: {e}")
        return False


if __name__ == "__main__":
    success = run_etl()
    sys.exit(0 if success else 1)
